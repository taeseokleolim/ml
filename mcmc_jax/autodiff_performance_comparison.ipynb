{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Model with NUTS with / without autodiff from JAX\n",
    "\n",
    "The details are from [here](https://cran.r-project.org/web/packages/hmclearn/vignettes/logistic_regression_hmclearn.html). \n",
    "\n",
    "The log posterior for logistic regression is given by the sum of the log likelihood and the log prior:\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{\\beta} | \\mathbf{y}, \\mathbf{X}) = \\log p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}) + \\log p(\\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "The log likelihood for logistic regression is given by:\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left[ y_i \\log \\left( \\frac{1}{1 + \\exp(-\\mathbf{x}_i^T \\boldsymbol{\\beta})} \\right) + (1 - y_i) \\log \\left( 1 - \\frac{1}{1 + \\exp(-\\mathbf{x}_i^T \\boldsymbol{\\beta})} \\right) \\right]\n",
    "$$\n",
    "\n",
    "The log prior for a Gaussian distribution is given by:\n",
    "\n",
    "$$\n",
    "\\log p(\\boldsymbol{\\beta}) = -\\frac{1}{2} \\boldsymbol{\\beta}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\beta} + \\text{const}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Sigma}$ is the covariance matrix of the Gaussian prior.\n",
    "\n",
    "The gradient of the log posterior with respect to $\\boldsymbol{\\beta}$ is given by the sum of the gradient of the log likelihood and the gradient of the log prior:\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(\\boldsymbol{\\beta} | \\mathbf{y}, \\mathbf{X}) = \\nabla \\log p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}) + \\nabla \\log p(\\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "The gradient of the log likelihood with respect to $\\boldsymbol{\\beta}$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{y} - \\mathbf{p})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{p} = (p_1, ..., p_n)^T$ and $p_i = 1 / (1 + \\exp(-\\mathbf{x}_i^T \\boldsymbol{\\beta}))$.\n",
    "\n",
    "The gradient of the log prior with respect to $\\boldsymbol{\\beta}$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla \\log p(\\boldsymbol{\\beta}) = -\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, vmap\n",
    "from jax.scipy.special import expit\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ModelwithNUTS(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prng_key = None\n",
    "        self.epsilon = None\n",
    "    \n",
    "    def log_likelihood(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def grad_ll(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def leapfrog(self, beta, p, epsilon):\n",
    "        p_half_step = p + epsilon / 2 * self.grad_ll(beta)\n",
    "        beta_new = beta + epsilon * p_half_step\n",
    "        p_new = p_half_step + epsilon / 2 * self.grad_ll(beta_new)\n",
    "        return beta_new, p_new\n",
    "    \n",
    "    def build_tree(self, u, v, j, epsilon, beta, p, r, Emax=1000):\n",
    "        if j == 0:\n",
    "            # Base case, take one leapfrog step in the direction v\n",
    "            beta_prime, p_prime = self.leapfrog(beta, p, v*epsilon)\n",
    "            if u <= jnp.exp(self.log_likelihood(beta_prime) - 0.5*jnp.dot(p_prime, p_prime)):\n",
    "                n_prime = 1\n",
    "            else:\n",
    "                n_prime = 0\n",
    "            s_prime = int(self.log_likelihood(beta_prime) - 0.5*jnp.dot(p_prime, p_prime) > u - Emax)\n",
    "            return beta_prime, p_prime, beta_prime, p_prime, beta_prime, n_prime, s_prime\n",
    "        else:\n",
    "            # Recursion, build left and right subtrees\n",
    "            beta_minus, p_minus, beta_plus, p_plus, beta_prime, n_prime, s_prime = self.build_tree(u, v, j-1, epsilon, beta, p, r)\n",
    "            if s_prime == 1:\n",
    "                if v == -1:\n",
    "                    beta_minus, p_minus, _, _, beta_double_prime, n_double_prime, s_double_prime = self.build_tree(u, v, j-1, epsilon, beta_minus, p_minus, r)\n",
    "                else:\n",
    "                    _, _, beta_plus, p_plus, beta_double_prime, n_double_prime, s_double_prime = self.build_tree(u, v, j-1, epsilon, beta_plus, p_plus, r)\n",
    "                if random.uniform(self.prng_key) < n_double_prime / max(n_prime + n_double_prime, 1):\n",
    "                    beta_prime = beta_double_prime\n",
    "                if jnp.dot(beta_plus-beta_minus, p_minus) >= 0 and jnp.dot(beta_plus-beta_minus, p_plus) >= 0:\n",
    "                    s_prime = s_double_prime\n",
    "                else:\n",
    "                    s_prime = 0\n",
    "                n_prime += n_double_prime\n",
    "            return beta_minus, p_minus, beta_plus, p_plus, beta_prime, n_prime, s_prime\n",
    "    \n",
    "    def NUTS(self, current_beta):\n",
    "        p = random.normal(self.prng_key, shape=current_beta.shape)\n",
    "        u = random.uniform(self.prng_key, minval=0, maxval=jnp.exp(self.log_likelihood(current_beta) - 0.5*jnp.dot(p, p)))\n",
    "        beta_minus = beta_plus = beta_prime = current_beta\n",
    "        p_minus = p_plus = p\n",
    "        j = 0\n",
    "        n_prime = s_prime = 1\n",
    "        r = 1e-10\n",
    "        while s_prime == 1:\n",
    "            v = random.choice(self.prng_key, a=jnp.array([-1, 1]))\n",
    "            if v == -1:\n",
    "                beta_minus, p_minus, _, _, beta_prime, n_prime, s_prime = self.build_tree(u, v, j, self.epsilon, beta_minus, p_minus, r)\n",
    "            else:\n",
    "                _, _, beta_plus, p_plus, beta_prime, n_prime, s_prime = self.build_tree(u, v, j, self.epsilon, beta_plus, p_plus, r)\n",
    "            r += n_prime\n",
    "            if s_prime == 1 and random.uniform(self.prng_key) < min(1, n_prime / r):\n",
    "                current_beta = beta_prime\n",
    "            j += 1\n",
    "        return current_beta\n",
    "    \n",
    "    \n",
    "class LogisticRegression(ModelwithNUTS):\n",
    "    def __init__(self, X, y, initial_beta, seed=0, epsilon=0.01):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.initial_beta = initial_beta\n",
    "        self.prng_key = random.PRNGKey(seed)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def log_likelihood(self, beta):\n",
    "        z = jnp.dot(self.X, beta)\n",
    "        return jnp.dot(self.y, z) - jnp.log(1 + jnp.exp(z)).sum()\n",
    "    \n",
    "    def grad_ll(self, beta):\n",
    "        return grad(self.log_likelihood)(beta)\n",
    "    \n",
    "    def fit(self, n_sample=100):\n",
    "        self.beta = [self.initial_beta]\n",
    "        for i in tqdm(range(n_sample)):\n",
    "            self.beta.append(self.NUTS(self.beta[-1]))\n",
    "        return self.beta\n",
    "    \n",
    "class LogisticRegression_noAutoDiff(ModelwithNUTS):\n",
    "    def __init__(self, X, y, initial_beta, seed=0, epsilon=0.01):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.initial_beta = initial_beta\n",
    "        self.prng_key = random.PRNGKey(seed)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def log_likelihood(self, beta):\n",
    "        z = jnp.dot(self.X, beta)\n",
    "        return jnp.dot(self.y, z) - jnp.log(1 + jnp.exp(z)).sum()\n",
    "    \n",
    "    def grad_ll(self, beta):\n",
    "        z = jnp.dot(self.X, beta)\n",
    "        return jnp.dot(self.X.T, (self.y - expit(z)))\n",
    "    \n",
    "    def fit(self, n_sample=100):\n",
    "        self.beta = [self.initial_beta]\n",
    "        for i in tqdm(range(n_sample)):\n",
    "            self.beta.append(self.NUTS(self.beta[-1]))\n",
    "        return self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "X = np.random.normal(size=(100, 2))\n",
    "X = np.hstack([np.ones((100, 1)), X])  # add intercept\n",
    "true_beta = np.array([0, 1, 2])\n",
    "y = np.random.binomial(1, sigmoid(X @ true_beta))\n",
    "\n",
    "initial_beta = np.zeros(X.shape[1])\n",
    "\n",
    "model = LogisticRegression(X, y, jnp.array(initial_beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:25<00:00,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 1min 16s, total: 4min 3s\n",
      "Wall time: 3min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = model.fit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression_noAutoDiff(X, y, jnp.array(initial_beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:51<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 1min 4s, total: 2min 14s\n",
      "Wall time: 1min 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = model.fit(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manatee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
